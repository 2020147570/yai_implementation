model
    attention
        single.py(Scaled Dot-Product Attention)
        ->
        attention/multi_head.py(Multi-Head Attention)
    embedding
        position.py(Positional Embedding),
        segment.py(Segment Embedding),
        token.py(Token Embedding)
        ->
        bert.py(BERT Embedding)
    utils
        gelu.py(GeLU activation function)
        ->
        feed_forward.py(Postionwise Feed Forward Network)
        ---
        layer_norm.py(Layer Normalization)
        ->
        sublayer.py(Sub Layer with Layer Normalization and Residual Connection)
attention(multi_head.py), utils(feed_forward.py, sublayer.py)
->
transformer.py(Transformer Encoder Block), embedding(bert.py)
->
bert.py(BERT model)
->
language_model.py(BERT Language Model with Masked Language Model and Next Sentence Prediction)
